{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://d0tz:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x20ca45271d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Importing PySpark and Initializing Spark\n",
    "import findspark\n",
    "findspark.init()  # This must be executed before importing PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "# Lets read the Multiline JSON\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing imports for 2015 and older data\n",
    "\n",
    "This is because the json files are saved differently and I wanted to see what could provide me the  correct import. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data count: 7097\n",
      "data1 count: 7437\n",
      "data2 count: 14534\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|name                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[Song of the Sea, Good Bread Alley, Wet Glitter., Freaking Awesome 2014 Season, Halfway through the Story of Our Life, William H. Seward vs. the Soviet Super-Men, The London Short Play Festival, Touretteshero: Backstage In Biscuit Land, A Lady Must Live, Propaganda! The Musical - NYMF 2014!, 'Unprescribed' goes to The Edinburgh Festival Fringe 2014, Ellis & Rose's Magikal Mystery Tour - with Phil Kay, Heartbeat Opera, The New Clubhouse, Save the Fairlee Drive-In!, Help Save the Historic Wilmette Theatre, PIONEER (Norfolk & Norwich Festival, 12th-14th May), Keep the Movie Memories coming to The Broadway Theatre, 2014 undergroundzero festival, Shakespeare's The Tempest: In-The-Round]                                                                                                                           |\n",
      "|[Halfway through the Story of Our Life, Wet Glitter., CHINESE COFFEE a play by Ira Lewis, Keep the Movie Memories coming to The Broadway Theatre, What Would Tina and Amy Do? at Captial Fringe 2014, PIONEER (Norfolk & Norwich Festival, 12th-14th May), Save the Fairlee Drive-In!, WORLD FAMOUS, A New Play, 2014 undergroundzero festival, Globe to Globe Hamlet, Early Days (of a better nation), DSI Comedy on Franklin Street, The Silence of the Lambs - THE MUSICAL???, Bitchtwitch: The Taking of Vicki Presh, Cabaret at The Merc Summer Series, The New Musical Tour \"Children In The Playground\", ObamAmerica Festival at Theatre503, Kids do \"Shrek\" for Kids!, Betsy: Wisdom of  a Brighton Whore, Renovate the Theater]                                                                                                     |\n",
      "|[Hamlet, Prince of Denmark, ACT TOO's \"The Phantom Tollbooth - The Musical\", Polk Outdoor Theatre, Get 'Beans on Toast' to the Edinburgh Fringe!, World's Smallest Drive-In Movie Theatre Goes Digital, Cabaret at The Merc Summer Series, Let's Tell the Story of the Soldier's Tale, Good Girls Get Paid, 'The People's Champion' goes to the Brighton Fringe., Luigi's Ladies, Irish Premier of Craig Adams' LIFT at the MAC Belfast, \"The Circle Belle\"...from bootlegging to stock car racing!, I (Heart) Brains, Celebrate Women! West Coast Premiere of IMPENETRABLE, The Silence of the Lambs - THE MUSICAL???, Plymouth Pilgrim / Tour Guide / History Teller, 2D's Creative Arts Present Spring Awakening, A night of mystery, murder... and comedy., T20 Artist Sponsorship Campaign, Save THE LAST JEWS - An Apocalyptic Comedy!]|\n",
      "|[The State Cinema's Update to Digital, Slotted Spoon Productions Presents Spelling Bee, I Survived, the play, A Serious Banquet, The Devil Without- an Immersive Theatre Experience, Getting Rid of Jaykunk...The Play, Wrinkle Writing Showcase, Sunny Side Up Drama: Summer Productions, Polk Outdoor Theatre, 2D's Creative Arts Present Spring Awakening, Liverpools fastest growing psychic night, A night of mystery, murder... and comedy., P.S. 84's Rendition of Disney's Frozen, Puppet show MONKEY's UK tour!, The Unscathed. A very unusual play, a Western., Godspell! The Broadway Musical!, Season 3: Youth Empowerment Performance Project, \"The Circle Belle\"...from bootlegging to stock car racing!, Next Stage Presents: \"Wiley and the Hairy Man\", Save THE LAST JEWS - An Apocalyptic Comedy!]                         |\n",
      "|[Scriptless Films Summer Screenings, Milner's Hall: Bringing People Together Since 1888, American Parent Trilogy, Four Noh Theatre Style Tragedies, Good Girls Get Paid, A Collection of Grimm's Fairy Tales, Sound Haven a place for Music, Learning, Gathering and Fun., Travesti - Edinburgh Festival '14, 'The People's Champion' goes to the Brighton Fringe., The State Cinema's Update to Digital, PE(T)ER GYNT, Journeys Beyond/Wac Arts at Edinburgh Fringe Festival, Slotted Spoon Productions Presents Spelling Bee, T20 Artist Sponsorship Campaign, Chaplin's Circus, YBGlobal Presents Shakespeare's Twelfth Night, Create the Good Gracious Show in Atlanta, P.S. 84's Rendition of Disney's Frozen, NEARLY REAL - SOLO THEATRE FESTIVAL - 2014, Get 'Beans on Toast' to the Edinburgh Fringe!]                               |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+\n",
      "|                name|\n",
      "+--------------------+\n",
      "|     Song of the Sea|\n",
      "|    Good Bread Alley|\n",
      "|        Wet Glitter.|\n",
      "|Freaking Awesome ...|\n",
      "|Halfway through t...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.json(r\"D:\\Data\\Kickstarter Data\\2015_and_older\\Kickstarter_2014-04-22.json\", multiLine = True)\n",
    "\n",
    "data1 = spark.read.json(r\"D:\\Data\\Kickstarter Data\\2015_and_older\\Kickstarter_2014-08-13.json\", multiLine = True, schema = data.schema)\n",
    "\n",
    "data2 = data.unionByName(data1, allowMissingColumns=True)\n",
    "\n",
    "print('data count:', data.count())\n",
    "print('data1 count:', data1.count())\n",
    "print('data2 count:', data2.select('projects').count())\n",
    "\n",
    "data2.select('projects.name').show(5, False)\n",
    "\n",
    "data2 = data2.withColumn('projects', explode('projects'))\n",
    "\n",
    "data2.select('projects.name').show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing recursively once I figured what was the correct way in which I could imported project level data. Also notice that I allow for missing columns because after 2014/12, they provide mroe columns which leads to more complexity when combining all prior years data with new JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First file: Kickstarter_2014-04-22.json processed\n",
      "Union Succesful for Kickstarter_2014-08-13.json\n",
      "Union Succesful for Kickstarter_2014-10-17.json\n",
      "Union Succesful for Kickstarter_2014-12-02.json\n",
      "Union Succesful for Kickstarter_2015-04-02.json\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "# Read the JSON files and union them\n",
    "first_file_processed = False\n",
    "# Loop through the files in the directory\n",
    "for dirname, _, filenames in os.walk(r\"D:\\Data\\Kickstarter Data\\2015_and_older\"):\n",
    "    for filename in filenames:\n",
    "        if not first_file_processed:\n",
    "            # Read the first file\n",
    "            data_2015 = spark.read.json(os.path.join(dirname, filename), multiLine = True)\n",
    "            # Set the flag to True\n",
    "            first_file_processed = True\n",
    "            print(f'First file: {filename} processed')\n",
    "        else:\n",
    "            # Read the next file and union it with the first file\n",
    "            df = spark.read.json(os.path.join(dirname, filename), multiLine = True, schema = data_2015.schema)\n",
    "            # Union the dataframes\n",
    "            data_2015 = data_2015.unionByName(df, allowMissingColumns=True)\n",
    "            print(f'Union Succesful for {filename}')\n",
    "\n",
    "data_2015 = data_2015.withColumn('projects', explode('projects'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- projects: struct (nullable = true)\n",
      " |    |-- backers_count: long (nullable = true)\n",
      " |    |-- blurb: string (nullable = true)\n",
      " |    |-- category: struct (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- parent_id: long (nullable = true)\n",
      " |    |    |-- position: long (nullable = true)\n",
      " |    |    |-- slug: string (nullable = true)\n",
      " |    |    |-- urls: struct (nullable = true)\n",
      " |    |    |    |-- web: struct (nullable = true)\n",
      " |    |    |    |    |-- discover: string (nullable = true)\n",
      " |    |-- country: string (nullable = true)\n",
      " |    |-- created_at: long (nullable = true)\n",
      " |    |-- creator: struct (nullable = true)\n",
      " |    |    |-- avatar: struct (nullable = true)\n",
      " |    |    |    |-- medium: string (nullable = true)\n",
      " |    |    |    |-- small: string (nullable = true)\n",
      " |    |    |    |-- thumb: string (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- slug: string (nullable = true)\n",
      " |    |    |-- urls: struct (nullable = true)\n",
      " |    |    |    |-- api: struct (nullable = true)\n",
      " |    |    |    |    |-- user: string (nullable = true)\n",
      " |    |    |    |-- web: struct (nullable = true)\n",
      " |    |    |    |    |-- user: string (nullable = true)\n",
      " |    |-- currency: string (nullable = true)\n",
      " |    |-- currency_symbol: string (nullable = true)\n",
      " |    |-- currency_trailing_code: boolean (nullable = true)\n",
      " |    |-- deadline: long (nullable = true)\n",
      " |    |-- disable_communication: boolean (nullable = true)\n",
      " |    |-- goal: double (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- launched_at: long (nullable = true)\n",
      " |    |-- location: struct (nullable = true)\n",
      " |    |    |-- country: string (nullable = true)\n",
      " |    |    |-- displayable_name: string (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- short_name: string (nullable = true)\n",
      " |    |    |-- slug: string (nullable = true)\n",
      " |    |    |-- state: string (nullable = true)\n",
      " |    |    |-- urls: struct (nullable = true)\n",
      " |    |    |    |-- api: struct (nullable = true)\n",
      " |    |    |    |    |-- nearby_projects: string (nullable = true)\n",
      " |    |    |    |-- web: struct (nullable = true)\n",
      " |    |    |    |    |-- discover: string (nullable = true)\n",
      " |    |    |    |    |-- location: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- photo: struct (nullable = true)\n",
      " |    |    |-- 1024x768: string (nullable = true)\n",
      " |    |    |-- 1536x1152: string (nullable = true)\n",
      " |    |    |-- ed: string (nullable = true)\n",
      " |    |    |-- full: string (nullable = true)\n",
      " |    |    |-- little: string (nullable = true)\n",
      " |    |    |-- med: string (nullable = true)\n",
      " |    |    |-- small: string (nullable = true)\n",
      " |    |    |-- thumb: string (nullable = true)\n",
      " |    |-- pledged: double (nullable = true)\n",
      " |    |-- slug: string (nullable = true)\n",
      " |    |-- state: string (nullable = true)\n",
      " |    |-- state_changed_at: long (nullable = true)\n",
      " |    |-- urls: struct (nullable = true)\n",
      " |    |    |-- web: struct (nullable = true)\n",
      " |    |    |    |-- project: string (nullable = true)\n",
      " |-- seed: string (nullable = true)\n",
      " |-- total_hits: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking the new schema\n",
    "data_2015.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing rest of 2015 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Union Succesful for Kickstarter_2015-11-01T14_09_04_557Z.json\n",
      "Union Succesful for Kickstarter_2015-12-17T12_09_06_107Z.json\n"
     ]
    }
   ],
   "source": [
    "# Rest of 2015\n",
    "for dirname, _, filenames in os.walk(r\"D:\\Data\\Kickstarter Data\\2015_new\"):\n",
    "    for filename in filenames:\n",
    "            # Read the first file\n",
    "            df = spark.read.json(os.path.join(dirname, filename)).\\\n",
    "                    withColumnRenamed('data', 'projects').\\\n",
    "                    drop('created_at',\n",
    "                        'id',\n",
    "                        'robot_id',\n",
    "                        'run_id',\n",
    "                        'table_id')\n",
    "            data_2015 = data_2015.unionByName(df, allowMissingColumns=True)\n",
    "            print(f'Union Succesful for {filename}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                name|\n",
      "+--------------------+\n",
      "|      Maridee Slater|\n",
      "|April Yvette Thom...|\n",
      "|        Lucile Scott|\n",
      "|  Three Day Hangover|\n",
      "|      Throes Theater|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check\n",
    "data_2015.select('projects.creator.name').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2016 old format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First file: Kickstarter_2016-01-28T09_15_08_781Z.json.gz processed\n",
      "Union Succesful for Kickstarter_2016-03-22T07_41_08_591Z.json.gz\n",
      "Union Succesful for Kickstarter_2016-04-15T02_09_04_328Z.json.gz\n",
      "Union Succesful for Kickstarter_2016-05-15T02_04_46_813Z.json.gz\n",
      "Union Succesful for Kickstarter_2016-06-15T02_04_49_697Z.json.gz\n",
      "Union Succesful for Kickstarter_2016-07-15T02_04_40_862Z.json.gz\n",
      "Union Succesful for Kickstarter_2016-08-15T02_04_03_829Z.json.gz\n",
      "Union Succesful for Kickstarter_2016-09-15T02_04_03_474Z.json.gz\n",
      "Union Succesful for Kickstarter_2016-12-15T22_20_52_411Z.json.gz\n"
     ]
    }
   ],
   "source": [
    "first_file_processed = False\n",
    "\n",
    "for dirname, _, filenames in os.walk(r\"D:\\Data\\Kickstarter Data\\2016\"):\n",
    "        for filename in filenames:\n",
    "            if not first_file_processed:\n",
    "                # Read the first file\n",
    "                data_2016 = spark.read.json(os.path.join(dirname, filename), dropFieldIfAllNull=True).\\\n",
    "                        withColumnRenamed('data', 'projects').\\\n",
    "                        drop('created_at',\n",
    "                            'id',\n",
    "                            'robot_id',\n",
    "                            'run_id',\n",
    "                            'table_id')\n",
    "                # Set the flag to True\n",
    "                first_file_processed = True\n",
    "                print(f'First file: {filenames[0]} processed')\n",
    "            else:\n",
    "                df = spark.read.json(os.path.join(dirname, filename), schema = data_2016.schema, dropFieldIfAllNull=True).\\\n",
    "                        withColumnRenamed('data', 'projects').\\\n",
    "                        drop('created_at',\n",
    "                            'id',\n",
    "                            'robot_id',\n",
    "                            'run_id',\n",
    "                            'table_id')\n",
    "                # Union the dataframes\n",
    "                data_2016 = data_2016.unionByName(df)\n",
    "                print(f'Union Succesful for {filename}')\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "data_2016 = data_2016.where(col(\"projects.creator.name\").isNotNull())\n",
    "\n",
    "data_2016.createOrReplaceTempView('data_2016')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking whether data is imported correctly with a count function. Doing that with spark is a good way of checking for incorrect imports. Usually this function breaks if there are null rows in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147429"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2016.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing 2016 data for simple repetition of creators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|                name|count(1)|\n",
      "+--------------------+--------+\n",
      "|         GBS Detroit|      94|\n",
      "|             Michael|      61|\n",
      "|         Game Salute|      58|\n",
      "|               James|      51|\n",
      "|              Daniel|      48|\n",
      "|               Chris|      48|\n",
      "|               David|      43|\n",
      "|                John|      41|\n",
      "|                Ryan|      40|\n",
      "|              Andrew|      36|\n",
      "|               Jason|      34|\n",
      "|Collectable Playi...|      34|\n",
      "|                Adam|      34|\n",
      "|         Queen Games|      33|\n",
      "|Gryphon and Eagle...|      32|\n",
      "|                Alex|      32|\n",
      "|                Matt|      29|\n",
      "|         Christopher|      28|\n",
      "|               Sarah|      28|\n",
      "|                Mike|      27|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql('''\n",
    "SELECT \n",
    "    projects.creator.name,\n",
    "    COUNT(*)\n",
    "FROM\n",
    "    data_2016\n",
    "GROUP BY\n",
    "    projects.creator.name\n",
    "ORDER BY \n",
    "    COUNT(*) DESC\n",
    "''')\n",
    "result.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- projects: struct (nullable = true)\n",
      " |    |-- backers_count: long (nullable = true)\n",
      " |    |-- blurb: string (nullable = true)\n",
      " |    |-- category: struct (nullable = true)\n",
      " |    |    |-- color: long (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- parent_id: long (nullable = true)\n",
      " |    |    |-- position: long (nullable = true)\n",
      " |    |    |-- slug: string (nullable = true)\n",
      " |    |    |-- urls: struct (nullable = true)\n",
      " |    |    |    |-- web: struct (nullable = true)\n",
      " |    |    |    |    |-- discover: string (nullable = true)\n",
      " |    |-- country: string (nullable = true)\n",
      " |    |-- created_at: long (nullable = true)\n",
      " |    |-- creator: struct (nullable = true)\n",
      " |    |    |-- avatar: struct (nullable = true)\n",
      " |    |    |    |-- medium: string (nullable = true)\n",
      " |    |    |    |-- small: string (nullable = true)\n",
      " |    |    |    |-- thumb: string (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- slug: string (nullable = true)\n",
      " |    |    |-- urls: struct (nullable = true)\n",
      " |    |    |    |-- api: struct (nullable = true)\n",
      " |    |    |    |    |-- user: string (nullable = true)\n",
      " |    |    |    |-- web: struct (nullable = true)\n",
      " |    |    |    |    |-- user: string (nullable = true)\n",
      " |    |-- currency: string (nullable = true)\n",
      " |    |-- currency_symbol: string (nullable = true)\n",
      " |    |-- currency_trailing_code: boolean (nullable = true)\n",
      " |    |-- deadline: long (nullable = true)\n",
      " |    |-- disable_communication: boolean (nullable = true)\n",
      " |    |-- goal: double (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- launched_at: long (nullable = true)\n",
      " |    |-- location: struct (nullable = true)\n",
      " |    |    |-- country: string (nullable = true)\n",
      " |    |    |-- displayable_name: string (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- is_root: boolean (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- short_name: string (nullable = true)\n",
      " |    |    |-- slug: string (nullable = true)\n",
      " |    |    |-- state: string (nullable = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |    |    |-- urls: struct (nullable = true)\n",
      " |    |    |    |-- api: struct (nullable = true)\n",
      " |    |    |    |    |-- nearby_projects: string (nullable = true)\n",
      " |    |    |    |-- web: struct (nullable = true)\n",
      " |    |    |    |    |-- discover: string (nullable = true)\n",
      " |    |    |    |    |-- location: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- photo: struct (nullable = true)\n",
      " |    |    |-- 1024x768: string (nullable = true)\n",
      " |    |    |-- 1536x1152: string (nullable = true)\n",
      " |    |    |-- ed: string (nullable = true)\n",
      " |    |    |-- full: string (nullable = true)\n",
      " |    |    |-- key: string (nullable = true)\n",
      " |    |    |-- little: string (nullable = true)\n",
      " |    |    |-- med: string (nullable = true)\n",
      " |    |    |-- small: string (nullable = true)\n",
      " |    |    |-- thumb: string (nullable = true)\n",
      " |    |-- pledged: double (nullable = true)\n",
      " |    |-- profile: struct (nullable = true)\n",
      " |    |    |-- background_color: string (nullable = true)\n",
      " |    |    |-- background_image_attributes: struct (nullable = true)\n",
      " |    |    |    |-- id: long (nullable = true)\n",
      " |    |    |    |-- image_urls: struct (nullable = true)\n",
      " |    |    |    |    |-- baseball_card: string (nullable = true)\n",
      " |    |    |    |    |-- default: string (nullable = true)\n",
      " |    |    |-- background_image_opacity: double (nullable = true)\n",
      " |    |    |-- blurb: string (nullable = true)\n",
      " |    |    |-- feature_image_attributes: struct (nullable = true)\n",
      " |    |    |    |-- id: long (nullable = true)\n",
      " |    |    |    |-- image_urls: struct (nullable = true)\n",
      " |    |    |    |    |-- baseball_card: string (nullable = true)\n",
      " |    |    |    |    |-- default: string (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- link_background_color: string (nullable = true)\n",
      " |    |    |-- link_text: string (nullable = true)\n",
      " |    |    |-- link_text_color: string (nullable = true)\n",
      " |    |    |-- link_url: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- project_id: long (nullable = true)\n",
      " |    |    |-- should_show_feature_image: boolean (nullable = true)\n",
      " |    |    |-- show_feature_image: boolean (nullable = true)\n",
      " |    |    |-- state: string (nullable = true)\n",
      " |    |    |-- state_changed_at: long (nullable = true)\n",
      " |    |    |-- text_color: string (nullable = true)\n",
      " |    |-- slug: string (nullable = true)\n",
      " |    |-- source_url: string (nullable = true)\n",
      " |    |-- spotlight: boolean (nullable = true)\n",
      " |    |-- staff_pick: boolean (nullable = true)\n",
      " |    |-- state: string (nullable = true)\n",
      " |    |-- state_changed_at: long (nullable = true)\n",
      " |    |-- static_usd_rate: string (nullable = true)\n",
      " |    |-- urls: struct (nullable = true)\n",
      " |    |    |-- web: struct (nullable = true)\n",
      " |    |    |    |-- project: string (nullable = true)\n",
      " |    |    |    |-- rewards: string (nullable = true)\n",
      " |    |-- usd_pledged: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_2016.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2017 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First file: Kickstarter_2017-01-15T22_21_04_985Z.json.gz processed\n",
      "Union Succesful for Kickstarter_2017-02-15T22_22_48_377Z.json.gz\n",
      "Union Succesful for Kickstarter_2017-03-15T22_20_55_874Z.json.gz\n",
      "Union Succesful for Kickstarter_2017-04-15T22_21_18_122Z.json.gz\n",
      "Union Succesful for Kickstarter_2017-05-15T22_21_11_300Z.json.gz\n",
      "Union Succesful for Kickstarter_2017-06-15T22_20_03_059Z.json.gz\n",
      "Union Succesful for Kickstarter_2017-07-15T22_20_48_951Z.json.gz\n",
      "Union Succesful for Kickstarter_2017-08-15T22_20_51_958Z.json.gz\n",
      "Union Succesful for Kickstarter_2017-09-15T22_20_48_432Z.json.gz\n",
      "Union Succesful for Kickstarter_2017-10-15T10_20_38_271Z.json.gz\n",
      "Union Succesful for Kickstarter_2017-11-15T10_21_04_919Z.json.gz\n",
      "Union Succesful for Kickstarter_2017-12-15T10_20_51_610Z.json.gz\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Read the JSON files and union them\n",
    "first_file_processed = False\n",
    "# Loop through the files in the directory\n",
    "for dirname, _, filenames in os.walk(r\"D:\\Data\\Kickstarter Data\\2017\"):\n",
    "    for filename in filenames:\n",
    "        if not first_file_processed:\n",
    "            # Read the first file\n",
    "            data_2017 = spark.read.json(os.path.join(dirname, filename), multiLine = True).\\\n",
    "                    withColumnRenamed('data', 'projects').\\\n",
    "                    drop('created_at',\n",
    "                        'id',\n",
    "                        'robot_id',\n",
    "                        'run_id',\n",
    "                        'table_id')\n",
    "            # Set the flag to True\n",
    "            first_file_processed = True\n",
    "            print(f'First file: {filename} processed')\n",
    "        else:\n",
    "            # Read the next file and union it with the first file\n",
    "            df = spark.read.json(os.path.join(dirname, filename), multiLine = True, schema = data_2017.schema).\\\n",
    "                    withColumnRenamed('data', 'projects').\\\n",
    "                    drop('created_at',\n",
    "                        'id',\n",
    "                        'robot_id',\n",
    "                        'run_id',\n",
    "                        'table_id')\n",
    "            # Union the dataframes\n",
    "            data_2017 = data_2017.union(df)\n",
    "            print(f'Union Succesful for {filename}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2018 data\n",
    "\n",
    "This dataset is slightly weird again. It has a whole column which is null. Therefore, a dropFieldIfAllNull is given here. This leads to increased import time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First file: Kickstarter_2018-01-12T10_20_09_196Z.json.gz processed\n",
      "Union Succesful for Kickstarter_2018-02-15T03_20_44_743Z.json.gz\n",
      "Union Succesful for Kickstarter_2018-03-15T03_20_39_033Z.json.gz\n",
      "Union Succesful for Kickstarter_2018-04-12T03_20_13_192Z.json.gz\n",
      "Union Succesful for Kickstarter_2018-05-17T03_20_08_333Z.json.gz\n",
      "Union Succesful for Kickstarter_2018-06-14T03_20_15_782Z.json.gz\n",
      "Union Succesful for Kickstarter_2018-07-12T03_20_16_435Z.json.gz\n",
      "Union Succesful for Kickstarter_2018-08-16T03_20_13_856Z.json.gz\n",
      "Union Succesful for Kickstarter_2018-09-13T03_20_17_777Z.json.gz\n",
      "Union Succesful for Kickstarter_2018-10-18T03_20_48_880Z.json.gz\n",
      "Union Succesful for Kickstarter_2018-11-15T03_20_50_568Z.json.gz\n",
      "Union Succesful for Kickstarter_2018-12-13T03_20_05_701Z.json.gz\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Read the JSON files and union them\n",
    "first_file_processed = False\n",
    "# Loop through the files in the directory\n",
    "for dirname, _, filenames in os.walk(r\"D:\\Data\\Kickstarter Data\\2018\"):\n",
    "    for filename in filenames:\n",
    "        if not first_file_processed:\n",
    "            # Read the first file\n",
    "            data_2018 = spark.read.json(os.path.join(dirname, filename), dropFieldIfAllNull=True).\\\n",
    "                    withColumnRenamed('data', 'projects').\\\n",
    "                    drop('created_at',\n",
    "                        'id',\n",
    "                        'robot_id',\n",
    "                        'run_id',\n",
    "                        'table_id')\n",
    "            # Set the flag to True\n",
    "            first_file_processed = True\n",
    "            print(f'First file: {filename} processed')\n",
    "        else:\n",
    "            # Read the next file and union it with the first file\n",
    "            df = spark.read.json(os.path.join(dirname, filename), dropFieldIfAllNull=True, schema = data_2018.schema).\\\n",
    "                    withColumnRenamed('data', 'projects').\\\n",
    "                    drop('created_at',\n",
    "                        'id',\n",
    "                        'robot_id',\n",
    "                        'run_id',\n",
    "                        'table_id')\n",
    "            # Union the dataframes\n",
    "            data_2018 = data_2018.unionByName(df, allowMissingColumns=True)\n",
    "            print(f'Union Succesful for {filename}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2019, 2020, 2021, 2022 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First file: Kickstarter_2019-01-17T03_20_02_630Z.json.gz processed\n",
      "Union Successful for Kickstarter_2019-02-14T03_20_04_734Z.json.gz\n",
      "Union Successful for Kickstarter_2019-03-14T03_20_12_200Z.json.gz\n",
      "Union Successful for Kickstarter_2019-04-18T03_20_02_220Z.json.gz\n",
      "Union Successful for Kickstarter_2019-05-16T03_20_20_822Z.json.gz\n",
      "Union Successful for Kickstarter_2019-06-13T03_20_35_801Z.json.gz\n",
      "Union Successful for Kickstarter_2019-07-18T03_20_05_009Z.json.gz\n",
      "Union Successful for Kickstarter_2019-08-15T03_20_03_022Z.json.gz\n",
      "Union Successful for Kickstarter_2019-09-12T03_20_06_215Z.json.gz\n",
      "Union Successful for Kickstarter_2019-10-17T03_20_19_421Z.json.gz\n",
      "Union Successful for Kickstarter_2019-11-14T03_20_27_004Z.json.gz\n",
      "Union Successful for Kickstarter_2019-12-12T03_20_05_306Z.json.gz\n",
      "First file: Kickstarter_2020-01-16T03_20_15_556Z.json.gz processed\n",
      "Union Successful for Kickstarter_2020-02-13T03_20_04_893Z.json.gz\n",
      "Union Successful for Kickstarter_2020-03-12T03_20_06_551Z.json.gz\n",
      "Union Successful for Kickstarter_2020-04-16T03_20_04_541Z.json.gz\n",
      "Union Successful for Kickstarter_2020-05-14T03_20_08_560Z.json.gz\n",
      "Union Successful for Kickstarter_2020-06-18T03_20_07_487Z.json.gz\n",
      "Union Successful for Kickstarter_2020-07-16T03_20_08_086Z.json.gz\n",
      "Union Successful for Kickstarter_2020-08-13T03_20_17_470Z.json.gz\n",
      "Union Successful for Kickstarter_2020-09-17T03_20_18_143Z.json.gz\n",
      "Union Successful for Kickstarter_2020-10-15T03_20_03_128Z.json.gz\n",
      "Union Successful for Kickstarter_2020-11-12T03_20_11_453Z.json.gz\n",
      "Union Successful for Kickstarter_2020-12-17T03_20_12_051Z.json.gz\n",
      "First file: Kickstarter_2021-01-14T03_20_05_328Z.json.gz processed\n",
      "Union Successful for Kickstarter_2021-02-11T03_20_07_976Z.json.gz\n",
      "Union Successful for Kickstarter_2021-03-18T03_20_11_507Z.json.gz\n",
      "Union Successful for Kickstarter_2021-04-15T03_20_08_451Z.json.gz\n",
      "Union Successful for Kickstarter_2021-05-13T03_20_03_813Z.json.gz\n",
      "Union Successful for Kickstarter_2021-06-17T03_20_03_179Z.json.gz\n",
      "Union Successful for Kickstarter_2021-07-15T03_20_57_837Z.json.gz\n",
      "Union Successful for Kickstarter_2021-08-12T03_20_04_185Z.json.gz\n",
      "Union Successful for Kickstarter_2021-09-16T03_20_11_333Z.json.gz\n",
      "Union Successful for Kickstarter_2021-10-14T03_20_07_518Z.json.gz\n",
      "Union Successful for Kickstarter_2021-11-18T03_20_04_819Z.json.gz\n",
      "Union Successful for Kickstarter_2021-12-09T03_20_13_242Z.json.gz\n",
      "First file: Kickstarter_2022-01-20T03_20_11_451Z.json.gz processed\n",
      "Union Successful for Kickstarter_2022-02-10T03_20_20_292Z.json.gz\n",
      "Union Successful for Kickstarter_2022-03-24T03_20_19_285Z.json.gz\n",
      "Union Successful for Kickstarter_2022-04-21T03_20_08_060Z.json.gz\n",
      "Union Successful for Kickstarter_2022-05-19T03_20_05_346Z.json.gz\n",
      "Union Successful for Kickstarter_2022-06-09T03_20_03_365Z.json.gz\n",
      "Union Successful for Kickstarter_2022-07-14T03_20_06_406Z.json.gz\n",
      "Union Successful for Kickstarter_2022-08-11T03_20_04_440Z.json.gz\n",
      "Union Successful for Kickstarter_2022-09-15T03_20_10_337Z.json.gz\n",
      "Union Successful for Kickstarter_2022-10-13T03_20_19_468Z.json.gz\n",
      "Union Successful for Kickstarter_2022-11-17T03_20_10_019Z.json.gz\n",
      "Union Successful for Kickstarter_2022-12-15T03_20_04_521Z.json.gz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def process_files_for_year(year):\n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder.appName(f'Kickstarter_{year}').getOrCreate()\n",
    "    \n",
    "    data = None\n",
    "    first_file_processed = False\n",
    "\n",
    "    directory = os.path.join(r\"D:\\Data\\Kickstarter Data\", str(year))\n",
    "    for dirname, _, filenames in os.walk(directory):\n",
    "        for filename in filenames:\n",
    "            file_path = os.path.join(dirname, filename)\n",
    "            df = spark.read.json(file_path, multiLine=True).\\\n",
    "                withColumnRenamed('data', 'projects').\\\n",
    "                drop('created_at', 'id', 'robot_id', 'run_id', 'table_id')\n",
    "\n",
    "            if not first_file_processed:\n",
    "                data = df\n",
    "                first_file_processed = True\n",
    "                print(f'First file: {filename} processed')\n",
    "            else:\n",
    "                data = data.unionByName(df, allowMissingColumns=True)\n",
    "                print(f'Union Successful for {filename}')\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Process files for each year\n",
    "data_2019 = process_files_for_year(2019)\n",
    "data_2020 = process_files_for_year(2020)\n",
    "data_2021 = process_files_for_year(2021)\n",
    "data_2022 = process_files_for_year(2022)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking data integrity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|          name|\n",
      "+--------------+\n",
      "|Maridee Slater|\n",
      "+--------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+---------------+\n",
      "|           name|\n",
      "+---------------+\n",
      "|Adelfino Corino|\n",
      "+---------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+--------------+\n",
      "|          name|\n",
      "+--------------+\n",
      "|Charlotte Cole|\n",
      "+--------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+------+\n",
      "|  name|\n",
      "+------+\n",
      "|arthur|\n",
      "+------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----------+\n",
      "|       name|\n",
      "+-----------+\n",
      "|Ryan Turner|\n",
      "+-----------+\n",
      "only showing top 1 row\n",
      "\n",
      "+---------------+\n",
      "|           name|\n",
      "+---------------+\n",
      "|Anh Vongbandith|\n",
      "+---------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+------------+\n",
      "|        name|\n",
      "+------------+\n",
      "|The Backyard|\n",
      "+------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----------+\n",
      "|       name|\n",
      "+-----------+\n",
      "|Jorge Muniz|\n",
      "+-----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [data_2015, data_2016, data_2017, data_2018, data_2019, data_2020, data_2021, data_2022]\n",
    "\n",
    "for i in data:\n",
    "    i.select('projects.creator.name').show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat-GPT prompt to predict gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write a python code to predict gender using an Open AI wrapper\n",
    "\n",
    "import openai\n",
    "\n",
    "client = openai.OpenAI(api_key=\"API KEY\", timeout=60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''responses = []\n",
    "\n",
    "for i in range(len(list_of_names)):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-0125\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a gender/company predictor bot. The following are list of names that you need to classify on whether they are male/female/company. Also provide the probability of the prediction with 4 decimal places. Format in guess/probability\"},\n",
    "                {\"role\": \"user\", \"content\": list_of_names[i]}]\n",
    "    )\n",
    "    responses.append(response.choices[0].message.content)\n",
    "\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#2019\n",
    "# Read the JSON files and union them\n",
    "first_file_processed = False\n",
    "# Loop through the files in the directory\n",
    "for dirname, _, filenames in os.walk(r\"D:\\Data\\Kickstarter Data\\2019\"):\n",
    "    for filename in filenames:\n",
    "        if not first_file_processed:\n",
    "            # Read the first file\n",
    "            data_2019 = spark.read.json(os.path.join(dirname, filename), multiLine = True).\\\n",
    "                    withColumnRenamed('data', 'projects').\\\n",
    "                    drop('created_at',\n",
    "                        'id',\n",
    "                        'robot_id',\n",
    "                        'run_id',\n",
    "                        'table_id')\n",
    "            # Set the flag to True\n",
    "            first_file_processed = True\n",
    "            print(f'First file: {filename} processed')\n",
    "        else:\n",
    "            # Read the next file and union it with the first file\n",
    "            df = spark.read.json(os.path.join(dirname, filename), multiLine = True).\\\n",
    "                    withColumnRenamed('data', 'projects').\\\n",
    "                    drop('created_at',\n",
    "                        'id',\n",
    "                        'robot_id',\n",
    "                        'run_id',\n",
    "                        'table_id')\n",
    "            # Union the dataframes\n",
    "            data_2019 = data_2019.unionByName(df, allowMissingColumns=True)\n",
    "            print(f'Union Succesful for {filename}')\n",
    "\n",
    "\n",
    "#2020\n",
    "\n",
    "# Read the JSON files and union them\n",
    "first_file_processed = False\n",
    "# Loop through the files in the directory\n",
    "for dirname, _, filenames in os.walk(r\"D:\\Data\\Kickstarter Data\\2020\"):\n",
    "    for filename in filenames:\n",
    "        if not first_file_processed:\n",
    "            # Read the first file\n",
    "            data_2020 = spark.read.json(os.path.join(dirname, filename), multiLine = True).\\\n",
    "                    withColumnRenamed('data', 'projects').\\\n",
    "                    drop('created_at',\n",
    "                        'id',\n",
    "                        'robot_id',\n",
    "                        'run_id',\n",
    "                        'table_id')\n",
    "            # Set the flag to True\n",
    "            first_file_processed = True\n",
    "            print(f'First file: {filename} processed')\n",
    "        else:\n",
    "            # Read the next file and union it with the first file\n",
    "            df = spark.read.json(os.path.join(dirname, filename), multiLine = True).\\\n",
    "                    withColumnRenamed('data', 'projects').\\\n",
    "                    drop('created_at',\n",
    "                        'id',\n",
    "                        'robot_id',\n",
    "                        'run_id',\n",
    "                        'table_id')\n",
    "            # Union the dataframes\n",
    "            data_2020 = data_2020.unionByName(df, allowMissingColumns=True)\n",
    "            print(f'Union Succesful for {filename}')\n",
    "\n",
    "\n",
    "\n",
    "#2021\n",
    "# Read the JSON files and union them\n",
    "first_file_processed = False\n",
    "# Loop through the files in the directory\n",
    "for dirname, _, filenames in os.walk(r\"D:\\Data\\Kickstarter Data\\2021\"):\n",
    "    for filename in filenames:\n",
    "        if not first_file_processed:\n",
    "            # Read the first file\n",
    "            data_2021 = spark.read.json(os.path.join(dirname, filename), multiLine = True).\\\n",
    "                    withColumnRenamed('data', 'projects').\\\n",
    "                    drop('created_at',\n",
    "                        'id',\n",
    "                        'robot_id',\n",
    "                        'run_id',\n",
    "                        'table_id')\n",
    "            # Set the flag to True\n",
    "            first_file_processed = True\n",
    "            print(f'First file: {filename} processed')\n",
    "        else:\n",
    "            # Read the next file and union it with the first file\n",
    "            df = spark.read.json(os.path.join(dirname, filename), multiLine = True).\\\n",
    "                    withColumnRenamed('data', 'projects').\\\n",
    "                    drop('created_at',\n",
    "                        'id',\n",
    "                        'robot_id',\n",
    "                        'run_id',\n",
    "                        'table_id')\n",
    "            # Union the dataframes\n",
    "            data_2021 = data_2021.unionByName(df, allowMissingColumns=True)\n",
    "            print(f'Union Succesful for {filename}')\n",
    "\n",
    "\n",
    "# 2022\n",
    "# Read the JSON files and union them\n",
    "first_file_processed = False\n",
    "# Loop through the files in the directory\n",
    "for dirname, _, filenames in os.walk(r\"D:\\Data\\Kickstarter Data\\2022\"):\n",
    "    for filename in filenames:\n",
    "        if not first_file_processed:\n",
    "            # Read the first file\n",
    "            data_2022 = spark.read.json(os.path.join(dirname, filename), multiLine = True).\\\n",
    "                    withColumnRenamed('data', 'projects').\\\n",
    "                    drop('created_at',\n",
    "                        'id',\n",
    "                        'robot_id',\n",
    "                        'run_id',\n",
    "                        'table_id')\n",
    "            # Set the flag to True\n",
    "            first_file_processed = True\n",
    "            print(f'First file: {filename} processed')\n",
    "        else:\n",
    "            # Read the next file and union it with the first file\n",
    "            df = spark.read.json(os.path.join(dirname, filename), multiLine = True).\\\n",
    "                    withColumnRenamed('data', 'projects').\\\n",
    "                    drop('created_at',\n",
    "                        'id',\n",
    "                        'robot_id',\n",
    "                        'run_id',\n",
    "                        'table_id')\n",
    "            # Union the dataframes\n",
    "            data_2022 = data_2022.unionByName(df, allowMissingColumns=True)\n",
    "            print(f'Union Succesful for {filename}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
